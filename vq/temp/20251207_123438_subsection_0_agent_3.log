```latex
\subsection{Encoder}

The Encoder is a pivotal component of our proposed framework, designed to transform raw images into effective latent representations suitable for quantization. Extracting significant features from the input images, the Encoder plays a critical role in improving the overall performance of the model. The latent representations generated by the Encoder serve as the input for the Vector Quantizer, an essential step in the architecture that facilitates dimensionality reduction while retaining important data characteristics.

\textbf{Input:} Raw images ($x$);  
\textbf{Output:} Latent representations ($z_e$).  

\textbf{Workflow:}  
\[
    x \xrightarrow{\text{Encoder}} z_e
\]

\subsubsection{Architecture}

The Encoder utilizes a Convolutional Neural Network (CNN) architecture that consists of multiple layers tasked with progressively extracting hierarchical spatial features from the images. Its key components include:

1. **Convolutional Layers**: These layers execute a series of convolutional operations to capture intricate spatial patterns in the input data. The initial layer processes the raw image, followed by several convolutional blocks that downsample the spatial dimensions while enhancing the depth of feature maps. The operation of the first convolutional layer can be mathematically expressed as:

   \[
   z_1 = \text{ReLU}(\text{Conv2D}(x)),
   \]

   where \(z_1\) represents the feature maps derived from the first convolutional layer acting on the raw image \(x\). Each subsequent layer refines these feature map representations, resulting in improved depth and quality of information extraction.

   \textbf{Input:} Raw images ($x$);  
   \textbf{Output:} Feature maps ($h$).  

   \textbf{Workflow:}  
   \[
   x \xrightarrow{\text{Convolutional Layers}} h
   \]

2. **Residual Stack**: To enhance the representational capacity of the Encoder and mitigate challenges related to gradient propagation, we incorporate a Residual Stack within the architecture. This structure employs skip connections, facilitating more efficient gradient flow during backpropagation, particularly in deeper networks. The transformation executed by the residual structure can be expressed as:

   \[
   z_e = h + F(h),
   \]

   where \(F(h)\) denotes the operations performed by the residual block on the feature maps \(h\). This design substantially alleviates the vanishing gradient problem, which is a common issue encountered in deep learning architectures.

   \textbf{Input:} Feature maps ($h$);  
   \textbf{Output:} Enhanced latent representations ($z_e$).  

   \textbf{Workflow:}  
   \[
   h \xrightarrow{\text{Residual Stack}} z_e
   \]

Through this multi-stage operational flow, the Encoder effectively captures critical latent attributes from raw input images, establishing a robust foundation for quantization in subsequent stages. The extraction of high-quality latent representations is vital for enhancing the fidelity and performance of the generative model.

Grounded in the theoretical insights from established literature \citep{NeuralDiscreteRepresentationLearning}, the Encoder adeptly navigates the complexities associated with quantizing high-dimensional data. Its architecture, featuring advanced elements such as residual connections and hierarchical convolutional layers, is meticulously crafted to promote efficient learning while maintaining high fidelity in the resultant image reconstructions. 

Additionally, the Encoder implements Gumbel-Softmax sampling to further enhance gradient flow, providing a sophisticated mechanism to regulate the encoding process during training, as articulated by the mathematical formulation:

\[
y = \text{softmax}\left(\frac{\text{logits} + gumbel}{\text{temperature}}\right),
\]

where \(gumbel\) is sampled from a Gumbel distribution, facilitating a smoother transition during the optimization of discrete representations. This feature further consolidates the Encoderâ€™s capability to optimize performance effectively while ensuring high-fidelity image reconstructions.
```