```latex
\subsection{Encoder}
The Encoder is a fundamental component of our proposed framework, responsible for transforming raw images into compact latent representations that are amenable to quantization. By effectively capturing and extracting significant features from the input images, the Encoder substantially contributes to the overall performance of the model. The latent representations generated by the Encoder serve as the input to the Vector Quantizer, a key stage in the architecture that facilitates dimensionality reduction while maintaining essential data characteristics.

\textbf{Input:} Raw images ($x$);  
\textbf{Output:} Latent representations ($z_e$).  

\textbf{Workflow:}  
\[
    x \xrightarrow{\text{Encoder}} z_e
\]

\subsubsection{Architecture}
The Encoder is built upon a Convolutional Neural Network (CNN) architecture, which comprises several layers designed to progressively capture hierarchical spatial features from the input images. The main components of the Encoder architecture include:

1. **Convolutional Layers**: These layers implement a series of convolutional operations to detect intricate spatial patterns within the input images. The initial layer processes the raw input image, followed by multiple convolutional blocks that downsample the spatial dimensions while augmenting the depth of the feature maps. The computation for the first convolutional layer can be mathematically expressed as follows:

   \[
   z_1 = \text{ReLU}(\text{Conv2D}(x)),
   \]

   where \(z_1\) denotes the feature maps generated after processing the raw image \(x\) through the initial convolutional layer. Each subsequent layer continues to refine the feature map representations.

   \textbf{Input:} Raw images ($x$);  
   \textbf{Output:} Feature maps ($h$).  

   \textbf{Workflow:}  
   \[
   x \xrightarrow{\text{Convolutional Layers}} h
   \]

2. **Residual Stack**: To enhance representational capacity and alleviate gradient propagation challenges, the Encoder incorporates a Residual Stack. This configuration utilizes skip connections that facilitate improved gradient flow during backpropagation, particularly in deeper network architectures. The residual transformations can be mathematically represented as follows:

   \[
   z_e = h + F(h),
   \]

   where \(F(h)\) indicates the operation performed by the residual block on the feature map \(h\), significantly aiding in mitigating the vanishing gradient problem.

   \textbf{Input:} Feature maps ($h$);  
   \textbf{Output:} Enhanced latent representations ($z_e$).  

   \textbf{Workflow:}  
   \[
   h \xrightarrow{\text{Residual Stack}} z_e
   \]

The multi-stage operational flow of the Encoder effectively captures critical latent features from the raw input images, establishing a robust foundation for quantization in the subsequent Vector Quantizer stage. The extraction of high-quality latent representations is pivotal for ensuring enhanced performance and fidelity in the generative model.

In line with the theoretical underpinnings outlined in previous works \citep{NeuralDiscreteRepresentationLearning}, the Encoder is adept at managing the complexities associated with the quantization of high-dimensional data. The Encoder's architecture, by incorporating advanced features such as residual connections and hierarchical feature extraction through convolutional layers, is designed to foster efficient learning while maintaining high fidelity in the resultant image reconstructions.
```