```latex
\section{Vector Quantized Variational AutoEncoders (VQ-VAEs)}
% This section presents the technical methodology of the proposed VQ-VAE framework.
% The framework employs convolutional encoders, a vector quantization mechanism, and decoders 
% to achieve effective image data compression and reconstruction. Key innovations include 
% residual learning, an exponential moving average codebook for stable training, and advanced 
% rotation-rescaling methods for improved gradient propagation.

% Input: Raw images (e.g., from the CIFAR-10 dataset).
% Output: Reconstructed images and quantization metrics.
% Workflow: Raw images $\rightarrow$ Encoder $\rightarrow$ Vector Quantizer $\rightarrow$ Decoder $\rightarrow$ Reconstructed images.

\subsection{Encoder}
% The encoder component maps input images to compressed latent representations. 
% It consists of convolutional layers and residual stacks that efficiently extract features required 
% for subsequent quantization. The latents generated serve as input for the vector quantization step.

% Input: Images of size $(C, H, W)$.
% Output: Latent representations of size $(D, H', W')$.
% Workflow: Input Images $\rightarrow$ Convolutional Layers $\rightarrow$ Residual Stacks $\rightarrow$ Latent Representations.

\subsubsection{Residual Block}
% This block enhances gradient flow through skip connections, allowing deeper networks to be trained 
% effectively. It improves feature capturing and contributes to the overall performance of the encoder.

% Input: Feature maps of size $(C, H, W)$.
% Output: Enhanced feature maps of the same dimensions.
% Workflow: Input Feature Maps $\rightarrow$ Convolutional Transformations $\rightarrow$ Residual Addition $\rightarrow$ Enhanced Feature Maps.

\subsubsection{Residual Stack}
% Composed of multiple residual blocks, this stack improves the richness of feature representations 
% while mitigating degradation issues in deeper networks, ensuring high-quality feature extraction.

% Input: Feature maps from the preceding layer of size $(C, H, W)$.
% Output: Enhanced feature maps processed through the stack.
% Workflow: Input Feature Maps $\rightarrow$ Multiple Residual Blocks $\rightarrow$ Enhanced Feature Maps.

\subsection{Vector Quantizer}
% The vector quantization component discretizes the continuous latent representations by selecting the 
% nearest entries from a learned codebook. This process stabilizes image reconstruction and offers 
% valuable feedback for encoder optimization.

% Input: Latent representations from the encoder.
% Output: Quantized representations and their corresponding indices.
% Workflow: Latent Representations $\rightarrow$ Nearest Codebook Indexing $\rightarrow$ EMA Updates $\rightarrow$ Quantized Outputs.

\subsubsection{EMA Codebook}
% Utilizing exponential moving averages, the EMA codebook promotes stable learning of discrete representations, 
% critical for effective quantization and preventing codebook collapse over time.

% Input: Encoder outputs and previous codebook embeddings.
% Output: Updated codebook entries for quantization.
% Workflow: Encodings $\rightarrow$ EMA Update Mechanism $\rightarrow$ Updated Codebook.

\subsubsection{RotationRescaleQuantizeFn}
% This function ensures robust gradient propagation during quantization by implementing rotation and 
% rescaling techniques, improving stability while maintaining alignment in gradient directions.

% Input: Encoded features (flattened) and their nearest codebook vectors.
% Output: Adjusted quantized vectors for stable optimization.
% Workflow: Inputs $\rightarrow$ Nearest Codebook Lookup $\rightarrow$ Rotation-Rescaling $\rightarrow$ Quantized Outputs.

\subsection{Decoder}
% The decoder reconstructs images from quantized latent representations, mirroring the encoder 
% architecture to effectively upscale features back to the original image dimensions while preserving fidelity.

% Input: Quantized latent representations from the vector quantizer.
% Output: Reconstructed images formatted to original dimensions.
% Workflow: Quantized Representations $\rightarrow$ Transposed Convolutions $\rightarrow$ Residual Stacks $\rightarrow$ Reconstructed Images.

\subsubsection{Transposed Convolutions}
% This operation remaps quantized latent representations to their original dimensions through transposed 
% convolutions, which are essential for learning spatial features during the reconstruction process.

% Input: Quantized latent vectors of size $(C, H', W')$.
% Output: Feature maps of size $(C, H, W)$ that match the original image dimensions.
% Workflow: Quantized Representations $\rightarrow$ Transposed Convolution Operations $\rightarrow$ Upscaled Feature Maps.

\subsubsection{Residual Stack in Decoder}
% The decoder employs a residual stack similar to that of the encoder, refining the features extracted 
% during encoding to ensure the final output accurately resembles the input images.

% Input: Feature maps derived from transposed convolution outputs.
% Output: Final output feature maps ready for image representation.
% Workflow: Input Feature Maps $\rightarrow$ Residual Block Transformations $\rightarrow$ Final Output Feature Maps.
```