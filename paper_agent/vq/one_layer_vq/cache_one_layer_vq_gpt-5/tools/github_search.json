{
    "name": "github_search",
    "args": {
        "metadata": {
            "source_papers": [
                {
                    "reference": "Neural discrete representation learning",
                    "rank": 1,
                    "type": [
                        "methodological foundation"
                    ],
                    "justification": "This paper introduces Vector Quantization (VQ) as a foundational technique for encoding data into discrete representations. It lays the methodological groundwork for unsupervised representation learning and latent generative models, which are central to the current research.",
                    "usage": "The core VQ method proposed in this study is directly utilized in the proposed model, providing the essential framework for vector quantization."
                },
                {
                    "reference": "Vector-quantized image modeling with improved VQGAN",
                    "rank": 2,
                    "type": [
                        "methodological foundation",
                        "critical component"
                    ],
                    "justification": "This work enhances the original VQ-VAE framework by integrating adversarial networks, thereby improving the perceptual quality of generated samples. It establishes a robust quantization protocol essential for effective latent generative modeling, which is critical for addressing codebook utilization in this paper.",
                    "usage": "The improved VQGAN methodology is built upon to develop the proposed model, particularly in optimizing codebook utilization without sacrificing model capacity."
                },
                {
                    "reference": "Taming transformers for high-resolution image synthesis",
                    "rank": 3,
                    "type": [
                        "methodological foundation",
                        "conceptual inspiration"
                    ],
                    "justification": "This study presents the proposed model, which combines Vector Quantization with adversarial training to achieve high-resolution image synthesis. It not only provides methodological advancements but also conceptually inspires the current research direction towards enhancing codebook utilization in VQ models.",
                    "usage": "VQGAN serves as a foundational model that the proposed model builds upon, especially in terms of integrating adversarial techniques to improve latent space optimization."
                },
                {
                    "reference": "Estimating or propagating gradients through stochastic neurons for conditional computation",
                    "rank": 4,
                    "type": [
                        "methodological foundation"
                    ],
                    "justification": "This study introduces the Straight-Through Estimator (STE), a crucial technique for enabling gradient propagation through non-differentiable operations in neural networks. STE is pivotal in training the proposed model by allowing gradients to flow through discrete decision points.",
                    "usage": "STE is employed in this study to facilitate gradient descent updates for the codebook vectors, ensuring effective training of the proposed model despite the discrete quantization step."
                },
                {
                    "reference": "Learning transferable visual models from natural language supervision.",
                    "rank": 5,
                    "type": [
                        "critical component"
                    ],
                    "justification": "This work leverages a pre-trained CLIP model to initialize the codebook, creating a well-structured latent space that aligns with encoder outputs. It demonstrates the effectiveness of using external models to enhance codebook utilization, which this study aims to improve upon without relying on such external dependencies.",
                    "usage": "VQGAN-LC, as proposed in this study, is used as a comparative baseline to highlight the limitations of relying on pre-trained models for codebook initialization."
                },
                {
                    "reference": "Finite scalar quantization: VQ-VAE made simple.",
                    "rank": 6,
                    "type": [
                        "critical component"
                    ],
                    "justification": "This paper introduces Finite Scalar Quantization (FSQ), a technique that reduces the dimensionality of the latent space to improve codebook utilization. While FSQ effectively addresses codebook collapse, it does so by compromising the proposed model capacity, a trade-off that this study seeks to overcome.",
                    "usage": "FSQ is evaluated as an existing method for mitigating representation collapse. The proposed model is proposed as a superior alternative that avoids the dimensionality reduction inherent in FSQ."
                },
                {
                    "reference": "Auto-encoding variational bayes.",
                    "rank": 7,
                    "type": [
                        "conceptual inspiration"
                    ],
                    "justification": "This foundational paper on Variational Autoencoders (VAEs) provides a contrasting perspective to VQ models by enforcing a Gaussian distribution on the latent space. It inspires the theoretical analysis in this study, particularly in understanding the disjoint optimization challenges inherent in VQ models.",
                    "usage": "Conceptual insights from VAEs are used to theoretically analyze the representation collapse problem in VQ models, highlighting the differences in optimization strategies between VAEs and the proposed approach."
                },
                {
                    "reference": "Categorical reparameterization with gumbel-softmax.",
                    "rank": 8,
                    "type": [
                        "conceptual inspiration"
                    ],
                    "justification": "This study presents the Gumbel-Softmax trick, an alternative quantization method that allows for differentiable sampling from categorical distributions. It serves as an inspiration for exploring various quantization strategies to improve codebook utilization in VQ models.",
                    "usage": "The Gumbel-Softmax technique is discussed as part of alternative quantization strategies, informing the development of the proposed model's approach to optimizing the latent space."
                }
            ],
            "task_instructions": "1. **Task**: The proposed model is designed to address representation collapse in Vector Quantized (VQ) models, specifically in unsupervised representation learning and latent generative models applicable to modalities like image and audio data.\n\n2. **Core Techniques/Algorithms**: The methodology introduces a linear transformation layer applied to the code vectors in a reparameterization strategy that leverages a learnable latent basis, enhancing the optimization of the entire codebook rather than individual code vectors.\n\n3. **Purpose and Function of Major Technical Components**:\n   - **Encoder (f_θ)**: Maps input data (images or audio) into a continuous latent representation (z_e).\n   - **Codebook (C)**: A collection of discrete code vectors used for quantizing the latent representations.\n   - **Linear Transformation Layer (W)**: A learnable matrix that transforms the codebook vectors, optimizing the entire latent space jointly to improve codebook utilization during training.\n   - **Decoder (g_ϕ)**: Reconstructs the input data from the quantized representations.\n\n4. **Implementation Details**:\n   - **Key Parameters**:\n     - Learning rate (η): Commonly set to 1e-4.\n     - Commitment weight (β): Adjust according to data modality, e.g., set to 1.0 for images and 1000.0 for audio.\n   - **Input/Output Specifications**:\n     - **Input**: Raw data instances, such as images of size 128x128 or audio frames. \n     - **Output**: Reconstructed data (images or audio).\n   - **Important Constraints**: The codebook size should be large enough to capture the data complexity; experiments indicate sizes like 65,536 or larger are beneficial.\n\n5. **Step-by-Step Description of Component Interaction**:\n   - **Step 1**: Initialize the codebook (C) using a distribution (e.g., Gaussian) and freeze its parameters for initial training iterations.\n   - **Step 2**: For each data instance (x), compute the latent representation (z_e) using the encoder (f_θ).\n   - **Step 3**: Perform nearest code search to find the closest codebook vector to z_e using the distance metric. Use the selected code vector for reconstruction.\n   - **Step 4**: Reparameterize the selected code vector using the performed linear transformation (C * W), effectively treating both C and W in the optimization process.\n   - **Step 5**: Calculate the loss, which combines reconstruction loss (MSE between original and decoded output) and commitment loss to ensure effective use of the codebook.\n   - **Step 6**: Update only the linear layer (W) through gradient backpropagation, keeping C static throughout this phase to facilitate the joint training procedure.\n\n6. **Critical Implementation Details**:\n   - To prevent representation collapse, it is crucial to carefully set the learning rate so that the transformation matrix W can adapt without compromising the usefulness of the latent space.\n   - Keeping the codebook static during the initial phase speeds up the convergence while ensuring that the linear transformation can stretch and rotate the latent space effectively.\n   - Regularly evaluate the utilization percentage of the codebook during training iterations, aiming for near-complete usage (ideally 100%) to combat representation collapse actively.",
            "date": "2024-01-01",
            "date_limit": "2024-01-01"
        }
    },
    "result": "The results of searching Neural discrete representation learning -user:lucidrains on GitHub: \n\n        Name: 1Konny/VQ-VAE\n        Description: Pytorch Implementation of \"Neural Discrete Representation Learning\"\n        Link: https://github.com/1Konny/VQ-VAE\n        Stars: 92\n        Created at: 2018-02-28T16:41:04Z\n        Language: Jupyter Notebook\n        \n        Name: hiwonjoon/tf-vqvae\n        Description: Tensorflow Implementation of the paper [Neural Discrete Representation Learning](https://arxiv.org/abs/1711.00937) (VQ-VAE).\n        Link: https://github.com/hiwonjoon/tf-vqvae\n        Stars: 263\n        Created at: 2017-11-10T01:12:51Z\n        Language: Jupyter Notebook\n        \n        Name: JeremyCCHsu/vqvae-speech\n        Description: Tensorflow implementation of the speech model described in Neural Discrete Representation Learning (a.k.a. VQ-VAE)\n        Link: https://github.com/JeremyCCHsu/vqvae-speech\n        Stars: 128\n        Created at: 2018-03-16T20:26:56Z\n        Language: Python\n        \n        Name: airalcorn2/vqvae-pytorch\n        Description: A minimal PyTorch implementation of the VQ-VAE model described in \"Neural Discrete Representation Learning\".\n        Link: https://github.com/airalcorn2/vqvae-pytorch\n        Stars: 84\n        Created at: 2022-01-29T20:08:45Z\n        Language: Python\n        \n        Name: liuxubo717/sound_generation\n        Description: Code and generated sounds for \"Conditional Sound Generation Using Neural Discrete Time-Frequency Representation Learning\", MLSP 2021\n        Link: https://github.com/liuxubo717/sound_generation\n        Stars: 69\n        Created at: 2021-03-18T18:11:13Z\n        Language: Python\n        \n        Name: pclucas14/vq-vae\n        Description: Pytorch implementation of \"Neural Discrete Representation Learning\"\n        Link: https://github.com/pclucas14/vq-vae\n        Stars: 8\n        Created at: 2019-06-15T13:53:22Z\n        Language: Python\n        \n        Name: soskek/vqvae_chainer\n        Description: Chainer's Neural Discrete Representation Learning (Aaron van den Oord et al., 2017)\n        Link: https://github.com/soskek/vqvae_chainer\n        Stars: 3\n        Created at: 2018-01-24T14:23:01Z\n        Language: Python\n        \n        Name: selforgmap/som-cpp\n        Description: Self Organizing Map (SOM) is a type of Artificial Neural Network (ANN) that is trained using an unsupervised, competitive learning to produce a low dimensional, discretized representation (feature map) of higher dimensional data.\n        Link: https://github.com/selforgmap/som-cpp\n        Stars: 5\n        Created at: 2019-02-23T14:53:00Z\n        Language: C++\n        \n        Name: IDSIA/kohonen-vae\n        Description: Official repository for the paper \"Topological Neural Discrete Representation Learning à la Kohonen\" (ICML 2023 Workshop on Sampling and Optimization in Discrete Space)\n        Link: https://github.com/IDSIA/kohonen-vae\n        Stars: 11\n        Created at: 2023-02-13T15:22:10Z\n        Language: Python\n        \n        Name: iomanker/VQVAE-TF2\n        Description: Implement paper for Neural Discrete Representation Learning. Code style is based on NVIDIA-lab.\n        Link: https://github.com/iomanker/VQVAE-TF2\n        Stars: 6\n        Created at: 2020-06-08T11:56:30Z\n        Language: Python\n        ******************************\nThe results of searching Vector-quantized image modeling with improved VQGAN -user:lucidrains on GitHub: \n******************************\nThe results of searching Taming transformers for high-resolution image synthesis -user:lucidrains on GitHub: \n\n        Name: CompVis/taming-transformers\n        Description: Taming Transformers for High-Resolution Image Synthesis\n        Link: https://github.com/CompVis/taming-transformers\n        Stars: 6367\n        Created at: 2020-12-17T14:47:06Z\n        Language: Jupyter Notebook\n        \n        Name: dome272/VQGAN-pytorch\n        Description: Pytorch implementation of VQGAN (Taming Transformers for High-Resolution Image Synthesis) (https://arxiv.org/pdf/2012.09841.pdf)\n        Link: https://github.com/dome272/VQGAN-pytorch\n        Stars: 540\n        Created at: 2022-02-15T11:38:32Z\n        Language: Python\n        \n        Name: Westlake-AI/VQGAN\n        Description: VQ-GAN for Various Data Modality based on Taming Transformers for High-Resolution Image Synthesis\n        Link: https://github.com/Westlake-AI/VQGAN\n        Stars: 27\n        Created at: 2023-04-13T15:47:05Z\n        Language: Python\n        \n        Name: Shubhamai/pytorch-vqgan\n        Description: This repo contains the implementation of VQGAN, Taming Transformers for High-Resolution Image Synthesis in PyTorch from scratch. I have added support for custom datasets, testings, experiment tracking etc.\n        Link: https://github.com/Shubhamai/pytorch-vqgan\n        Stars: 38\n        Created at: 2022-08-13T11:24:31Z\n        Language: Python\n        \n        Name: rosinality/taming-transformers-pytorch\n        Description: Implementation of Taming Transformers for High-Resolution Image Synthesis (https://arxiv.org/abs/2012.09841) in PyTorch\n        Link: https://github.com/rosinality/taming-transformers-pytorch\n        Stars: 16\n        Created at: 2020-12-28T06:43:55Z\n        Language: None\n        \n        Name: Vrushank264/VQGAN\n        Description: Pytorch implementation of \"Taming transformer for high resolution image synthesis (VQGAN)\"\n        Link: https://github.com/Vrushank264/VQGAN\n        Stars: 2\n        Created at: 2023-01-12T16:08:58Z\n        Language: Python\n        \n        Name: OccupyMars2025/Taming-Transformers-for-High-Resolution-Image-Synthesis\n        Description: None\n        Link: https://github.com/OccupyMars2025/Taming-Transformers-for-High-Resolution-Image-Synthesis\n        Stars: 0\n        Created at: 2022-06-06T18:29:33Z\n        Language: None\n        \n        Name: HiroForYou/Image-Synthesis-with-Transformers\n        Description: Taming Transformers for High-Resolution Image Synthesis\n        Link: https://github.com/HiroForYou/Image-Synthesis-with-Transformers\n        Stars: 1\n        Created at: 2021-10-09T03:26:41Z\n        Language: Jupyter Notebook\n        \n        Name: Grozby/vqgan\n        Description: Keras implementation of \"Taming Transformers for High-Resolution Image Synthesis\", https://arxiv.org/pdf/2012.09841.pdf\n        Link: https://github.com/Grozby/vqgan\n        Stars: 2\n        Created at: 2023-05-29T22:03:49Z\n        Language: Jupyter Notebook\n        \n        Name: tanmayj2020/taming_transformer\n        Description: Pytorch Implementation of Taming transformer for high resolution image synthesis\n        Link: https://github.com/tanmayj2020/taming_transformer\n        Stars: 0\n        Created at: 2022-04-18T10:29:06Z\n        Language: None\n        ******************************\nThe results of searching Estimating or propagating gradients through stochastic neurons for conditional computation -user:lucidrains on GitHub: \n******************************\nThe results of searching Learning transferable visual models from natural language supervision. -user:lucidrains on GitHub: \n\n        Name: leaderj1001/CLIP\n        Description: CLIP: Connecting Text and Image (Learning Transferable Visual Models From Natural Language Supervision)\n        Link: https://github.com/leaderj1001/CLIP\n        Stars: 82\n        Created at: 2021-01-11T00:38:08Z\n        Language: Python\n        \n        Name: ExcelsiorCJH/CLIP\n        Description: CLIP: Learning Transferable Visual Models From Natural Language Supervision\n        Link: https://github.com/ExcelsiorCJH/CLIP\n        Stars: 6\n        Created at: 2023-11-26T23:47:08Z\n        Language: Jupyter Notebook\n        \n        Name: SZU-AdvTech-2022/175-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\n        Description: None\n        Link: https://github.com/SZU-AdvTech-2022/175-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision\n        Stars: 1\n        Created at: 2023-03-03T17:49:13Z\n        Language: Jupyter Notebook\n        \n        Name: andregaio/clip\n        Description: A PyTorch implementation of 'Learning Transferable Visual Models From Natural Language Supervision' [2021]\n        Link: https://github.com/andregaio/clip\n        Stars: 1\n        Created at: 2023-12-10T14:20:45Z\n        Language: Python\n        ******************************\nThe results of searching Finite scalar quantization: VQ-VAE made simple. -user:lucidrains on GitHub: \n\n        Name: Nikolai10/FSQ\n        Description: TensorFlow implementation of \"Finite Scalar Quantization: VQ-VAE Made Simple\" (ICLR 2024)\n        Link: https://github.com/Nikolai10/FSQ\n        Stars: 21\n        Created at: 2023-12-02T18:57:54Z\n        Language: Python\n        ******************************\nThe results of searching Auto-encoding variational bayes. -user:lucidrains on GitHub: \n\n        Name: peiyunh/mat-vae\n        Description: A MATLAB implementation of Auto-Encoding Variational Bayes\n        Link: https://github.com/peiyunh/mat-vae\n        Stars: 48\n        Created at: 2016-06-06T20:12:47Z\n        Language: Matlab\n        \n        Name: nitarshan/variational-autoencoder\n        Description: PyTorch implementation of \"Auto-Encoding Variational Bayes\"\n        Link: https://github.com/nitarshan/variational-autoencoder\n        Stars: 42\n        Created at: 2017-03-22T23:56:20Z\n        Language: Jupyter Notebook\n        \n        Name: kuc2477/pytorch-vae\n        Description: PyTorch implementation of \"Auto-Encoding Variational Bayes\", arxiv:1312.6114\n        Link: https://github.com/kuc2477/pytorch-vae\n        Stars: 57\n        Created at: 2017-10-22T08:39:03Z\n        Language: Python\n        \n        Name: cshenton/auto-encoding-variational-bayes\n        Description: Replication of \"Auto-Encoding Variational Bayes\" (Kingma & Welling, 2013)\n        Link: https://github.com/cshenton/auto-encoding-variational-bayes\n        Stars: 19\n        Created at: 2018-02-27T06:35:39Z\n        Language: Python\n        \n        Name: dillonalaird/VAE\n        Description: Tensorflow implementation of Auto-Encoding Variational Bayes\n        Link: https://github.com/dillonalaird/VAE\n        Stars: 8\n        Created at: 2016-11-27T22:10:55Z\n        Language: Python\n        \n        Name: omarnmahmood/AEVB\n        Description: Auto-Encoding Variational Bayes\n        Link: https://github.com/omarnmahmood/AEVB\n        Stars: 7\n        Created at: 2018-02-06T13:05:19Z\n        Language: Jupyter Notebook\n        \n        Name: romain-lopez/HCV\n        Description: Information Constraints on Auto-Encoding Variational Bayes\n        Link: https://github.com/romain-lopez/HCV\n        Stars: 11\n        Created at: 2018-05-24T14:38:21Z\n        Language: Python\n        \n        Name: DongjunLee/vae-tensorflow\n        Description: TensorFlow implementation of Auto-Encoding Variational Bayes.\n        Link: https://github.com/DongjunLee/vae-tensorflow\n        Stars: 8\n        Created at: 2018-01-30T11:51:56Z\n        Language: Python\n        \n        Name: PrateekMunjal/-Auto-Encoding-Variational-Bayes-aka-VAE\n        Description: A tensorflow implementation of Variational autoencoder. We present the results on real world datasets, namely; celebA and Mnist dataset.\n        Link: https://github.com/PrateekMunjal/-Auto-Encoding-Variational-Bayes-aka-VAE\n        Stars: 5\n        Created at: 2019-01-12T06:11:49Z\n        Language: Python\n        \n        Name: NoviceStone/VAE\n        Description: Paper Reimplementation —— \"D. P. Kingma and M. Welling. Auto-Encoding Variational Bayes. ICLR, 2014.\"\n        Link: https://github.com/NoviceStone/VAE\n        Stars: 17\n        Created at: 2019-12-14T08:52:04Z\n        Language: Python\n        ******************************\nThe results of searching Categorical reparameterization with gumbel-softmax. -user:lucidrains on GitHub: \n\n        Name: Jasonlee1995/Gumbel_Softmax\n        Description: Unofficial Pytorch implementation of the paper 'Categorical Reparameterization with Gumbel-Softmax' and 'The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables'\n        Link: https://github.com/Jasonlee1995/Gumbel_Softmax\n        Stars: 11\n        Created at: 2021-03-29T02:38:21Z\n        Language: Jupyter Notebook\n        \n        Name: syyunn/Categorical-Reparameterization-with-Gumbel-Softmax\n        Description: Visual proof of Gumbel-Softmax distribution approximating categorical distribution \n        Link: https://github.com/syyunn/Categorical-Reparameterization-with-Gumbel-Softmax\n        Stars: 1\n        Created at: 2020-01-07T03:18:11Z\n        Language: None\n        ******************************\n"
}