***Directory Tree***:
[{'path': '', 'files': ['run_ablation_rrt.py', 'run_training_testing.py'], 'dirs': ['data_processing', 'model', 'training', 'results', 'data', 'testing']}, {'path': 'data_processing', 'files': ['cifar10.py'], 'dirs': []}, {'path': 'model', 'files': ['vector_quantizer.py', 'utils.py', 'vqvae.py'], 'dirs': []}, {'path': 'training', 'files': ['train.py'], 'dirs': []}, {'path': 'results', 'files': ['logs.json', 'ablation_rrt.json'], 'dirs': []}, {'path': 'data', 'files': ['cifar-10-python.tar.gz'], 'dirs': ['cifar-10-batches-py']}, {'path': 'data/cifar-10-batches-py', 'files': ['test_batch', 'data_batch_4', 'batches.meta', 'readme.html', 'data_batch_1', 'data_batch_3', 'data_batch_5', 'data_batch_2'], 'dirs': []}, {'path': 'testing', 'files': ['fid.py', 'test.py'], 'dirs': []}]

***Code Contents***:
[{'path': 'run_ablation_rrt.py', 'content': '"""\nRun ablation comparing RRT angle-preserving gradient transport vs standard straight-through.\nTrains each setting for 2 epochs on CIFAR-10 and evaluates MSE, PSNR, SSIM, FID.\nLogs results to /workplace/project/results/ablation_rrt.json.\n"""\nimport os\nimport json\nimport torch\nimport torch.optim as optim\n\nfrom model.vqvae import VQVAE\nfrom data_processing.cifar10 import get_cifar10_loaders\nfrom training.train import train_one_epoch\nfrom testing.test import evaluate\n\n\ndef run_ablation():\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    results = {}\n    os.makedirs(\'/workplace/project/results\', exist_ok=True)\n\n    config = {\n        \'batch_size\': 128,\n        \'num_workers\': 2,\n        \'codebook_size\': 1024,\n        \'embedding_dim\': 64,\n        \'lr\': 2e-4,\n        \'epochs\': 2,\n        \'ema_decay\': 0.99,\n        \'beta\': 0.5,\n    }\n\n    train_loader, test_loader = get_cifar10_loaders(batch_size=config[\'batch_size\'], num_workers=config[\'num_workers\'])\n\n    for use_rrt in [True, False]:\n        tag = \'rrt\' if use_rrt else \'straight_through\'\n        model = VQVAE(img_channels=3,\n                      codebook_size=config[\'codebook_size\'],\n                      beta=config[\'beta\'],\n                      embedding_dim=config[\'embedding_dim\'],\n                      ema_decay=config[\'ema_decay\'],\n                      use_rrt=use_rrt).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=config[\'lr\'])\n\n        train_logs = []\n        for epoch in range(1, config[\'epochs\'] + 1):\n            stats = train_one_epoch(model, train_loader, optimizer, device)\n            train_logs.append(stats)\n        test_stats = evaluate(model, test_loader, device)\n        results[tag] = {\n            \'train\': train_logs,\n            \'test\': test_stats,\n        }\n\n    with open(\'/workplace/project/results/ablation_rrt.json\', \'w\') as f:\n        json.dump(results, f, indent=2)\n\n\nif __name__ == \'__main__\':\n    run_ablation()\n'}, {'path': 'run_training_testing.py', 'content': '"""\nMain script to run training and testing for VQ-VAE with Rotation-Rescaling Transform.\n\nImplements:\n- Data loading (CIFAR-10)\n- Model assembly\n- Two epochs of training\n- Evaluation (FID and reconstruction loss)\n- Results logging\n"""\nimport os\nimport json\nimport torch\nimport torch.optim as optim\n\nfrom model.vqvae import VQVAE\nfrom data_processing.cifar10 import get_cifar10_loaders\nfrom training.train import train_one_epoch\nfrom testing.test import evaluate\n\ndef main():\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    print(f"Using device: {device}")\n\n    # Config\n    config = {\n        \'batch_size\': 128,\n        \'num_workers\': 2,\n        \'codebook_size\': 1024,\n        \'embedding_dim\': 64,\n        \'beta\': 0.5,\n        \'ema_decay\': 0.99,\n        \'lr\': 2e-4,\n        \'epochs\': 3,\n    }\n\n    # Data\n    train_loader, test_loader = get_cifar10_loaders(batch_size=config[\'batch_size\'], num_workers=config[\'num_workers\'])\n\n    # Model\n    model = VQVAE(img_channels=3,\n                  codebook_size=config[\'codebook_size\'],\n                  beta=config[\'beta\'],\n                  embedding_dim=config[\'embedding_dim\'],\n                  ema_decay=config[\'ema_decay\']).to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=config[\'lr\'])\n\n    # Training\n    logs = {\'train\': [], \'test\': None}\n    for epoch in range(1, config[\'epochs\'] + 1):\n        print(f"Epoch {epoch}/{config[\'epochs\']}")\n        train_stats = train_one_epoch(model, train_loader, optimizer, device)\n        print("Train:", train_stats)\n        logs[\'train\'].append(train_stats)\n\n    # Testing\n    test_stats = evaluate(model, test_loader, device)\n    print("Test:", test_stats)\n    logs[\'test\'] = test_stats\n\n    os.makedirs(\'/workplace/project/results\', exist_ok=True)\n    with open(\'/workplace/project/results/logs.json\', \'w\') as f:\n        json.dump(logs, f, indent=2)\n\nif __name__ == \'__main__\':\n    main()\n'}, {'path': 'data_processing/cifar10.py', 'content': '"""\nCIFAR-10 data processing pipeline.\n\nDownloads the dataset to /workplace/project/data if not present, applies standard\ntransforms, and provides DataLoaders for training and testing.\n\nFID evaluation resizes images to 299x299 for InceptionV3 features.\n"""\nimport os\nfrom typing import Tuple\nimport torch\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as T\n\nDATA_DIR = "/workplace/project/data"\n\n\ndef get_cifar10_loaders(batch_size: int = 128, num_workers: int = 2) -> Tuple[DataLoader, DataLoader]:\n    os.makedirs(DATA_DIR, exist_ok=True)\n    # Transforms for VQ-VAE: [0,1]\n    transform = T.Compose([\n        T.ToTensor(),\n    ])\n    trainset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=True, download=True, transform=transform)\n    testset = torchvision.datasets.CIFAR10(root=DATA_DIR, train=False, download=True, transform=transform)\n\n    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n    return trainloader, testloader\n'}, {'path': 'model/vector_quantizer.py', 'content': '"""\nVector Quantizer with Exponential Moving Average (EMA) updates and\nRotation-Rescaling Transform (RRT) straight-through gradient routing.\n\nOrigins & References:\n- Neural Discrete Representation Learning (van den Oord et al., 2017)\n- airalcorn2-vqvae-pytorch (reimplemented logic for EMA updates and losses)\n- This project integrates a novel gradient transport based on Householder rotation\n  and rescaling, as defined in utils.RRTStraightThrough.\n\nAll code is rewritten and self-contained; no external imports from reference repos.\n"""\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .utils import RRTStraightThrough, measure_perplexity\n\nclass VectorQuantizerEMA(nn.Module):\n    """\n    VQ module implementing codebook with EMA updates and commitment loss.\n\n    Forward returns the quantized vectors e_q, but we register a custom backward\n    via RRTStraightThrough to transport gradients through quantization preserving\n    angle with codebook vector, if use_rrt=True. Otherwise, uses standard straight-through.\n    """\n\n    def __init__(self, embedding_dim: int, num_embeddings: int,\n                 decay: float = 0.99, epsilon: float = 1e-5,\n                 beta: float = 0.25, use_rrt: bool = True):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.decay = decay\n        self.epsilon = epsilon\n        self.beta = beta\n        self.use_rrt = use_rrt\n\n        # Codebook embeddings (dictionary) e_i in R^{K x D}\n        limit = 3 ** 0.5\n        embed = torch.empty(num_embeddings, embedding_dim, dtype=torch.float32).uniform_(-limit, limit)\n        self.register_buffer(\'embedding\', embed)\n        # EMA buffers following Sonnet\'s implementation (counts and sums)\n        self.register_buffer(\'cluster_size\', torch.zeros(num_embeddings, dtype=torch.float32))\n        self.register_buffer(\'embed_avg\', embed.clone())\n\n    @torch.no_grad()\n    def _ema_update(self, z: torch.Tensor, encoding_indices: torch.Tensor):\n        """\n        Sonnet-style EMA updates with Laplace smoothing and dead-code reinitialization.\n        z: [N, D] on same device as embedding\n        encoding_indices: [N]\n        """\n        device = z.device\n        # Cast z to codebook dtype (float32) to avoid AMP dtype mismatches\n        z = z.to(self.embed_avg.dtype)\n        K = self.num_embeddings\n        # One-hot encodings [N, K]\n        encodings = F.one_hot(encoding_indices, K).to(self.embed_avg.dtype)  # [N, K]\n        # Current batch stats\n        cluster_size_batch = encodings.sum(0)  # [K]\n        embed_sum_batch = encodings.t() @ z  # [K, D]\n\n        # Exponential moving average update\n        self.cluster_size.mul_(self.decay).add_(cluster_size_batch * (1.0 - self.decay))\n        self.embed_avg.mul_(self.decay).add_(embed_sum_batch * (1.0 - self.decay))\n\n        # Laplace smoothing of counts to avoid zeros\n        n = self.cluster_size.sum()\n        cluster_size = (self.cluster_size + self.epsilon)\n        # Renormalize counts to sum to n (as in Sonnet)\n        cluster_size = cluster_size / cluster_size.sum().clamp_min(1e-8) * n\n\n        # Normalized embeddings\n        embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n\n        # Dead code handling: reinitialize rarely used codes from random z\n        dead_codes = (cluster_size < 1.0)\n        if torch.any(dead_codes) and z.shape[0] > 0:\n            num_dead = int(dead_codes.sum().item())\n            rand_idx = torch.randint(0, z.shape[0], (num_dead,), device=device)\n            self.embed_avg[dead_codes] = z[rand_idx]\n            cluster_size[dead_codes] = 1.0\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(1)\n\n        # In-place update to preserve buffer identity\n        self.embedding.copy_(embed_normalized)\n\n    def forward(self, z_e: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        """\n        Args:\n            z_e: [B, D, H, W] or [N, D] encoder outputs.\n        Returns:\n            e_q: quantized vectors same shape as z_e.\n            indices: selected code indices.\n            vq_loss: commitment + codebook loss for training stability.\n            stats: perplexity and cluster_use for logging\n        """\n        # Flatten spatial dims if present\n        orig_shape = z_e.shape\n        if z_e.dim() == 4:\n            B, D, H, W = orig_shape\n            z = z_e.permute(0, 2, 3, 1).contiguous().view(-1, D)\n        elif z_e.dim() == 2:\n            z = z_e\n            D = z.shape[-1]\n            B = None\n        else:\n            raise ValueError(f"Unsupported shape for z_e: {orig_shape}")\n\n        # Compute distances to codebook\n        embed = self.embedding  # [K, D], float32 buffer\n        # Cast z to embedding dtype for distance computation stability\n        z32 = z.to(embed.dtype)\n        z_sq = torch.sum(z32 ** 2, dim=1, keepdim=True)  # [N, 1]\n        e_sq = torch.sum(embed ** 2, dim=1)  # [K]\n        ze = z32 @ embed.t()  # [N, K]\n        distances = z_sq + e_sq.unsqueeze(0) - 2 * ze\n        # Choose nearest embedding\n        indices = torch.argmin(distances, dim=1)\n        e_q_flat = F.embedding(indices, embed)  # [N, D] float32\n\n        # Update codebook via EMA (stop-grad)\n        self._ema_update(z.detach(), indices.detach())\n\n        # Losses (treat decoder input as constants for encoder path)\n        # Compute losses in float32 for stability\n        codebook_loss = F.mse_loss(e_q_flat, z32.detach())\n        commit_loss = self.beta * F.mse_loss(z32, e_q_flat.detach())\n        vq_loss = codebook_loss + commit_loss\n\n        # Straight-through gradient handling\n        if self.use_rrt:\n            # RRT; forward remains e_q, backward transports angle-preserving grad\n            e_q_st = e_q_flat.to(z.dtype)\n            e_q_st = RRTStraightThrough.apply(z, e_q_st)\n        else:\n            # Standard straight-through estimator: treat quantization as identity in backward\n            # e_q = z + (e_q_flat - z).detach()\n            e_q_st = z + (e_q_flat.to(z.dtype) - z).detach()\n\n        # Reshape back\n        if z_e.dim() == 4:\n            e_q = e_q_st.view(B, H, W, D).permute(0, 3, 1, 2).contiguous()\n        else:\n            e_q = e_q_st\n\n        # Stats\n        perplexity, cluster_use = measure_perplexity(indices, self.num_embeddings)\n        stats = torch.stack([perplexity, cluster_use.float()])\n        return e_q, indices, vq_loss, stats\n'}, {'path': 'model/utils.py', 'content': '"""\nUtility functions and autograd helpers for the VQ-VAE with Rotation-Rescaling Transform (RRT).\n\nThis file integrates ideas adapted from multiple references:\n- Neural Discrete Representation Learning (van den Oord et al., 2017)\n- Categorical Reparameterization with Gumbel-Softmax (Jang et al., 2017)\n- Reference implementations: airalcorn2-vqvae-pytorch, CompVis latent-diffusion\n\nAll code is rewritten to fit a self-contained architecture and documented with origins and modifications.\n"""\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef gumbel_softmax_sample(logits: torch.Tensor, temperature: float, device: torch.device = torch.device("cpu")) -> torch.Tensor:\n    """\n    Sample from Gumbel-Softmax distribution.\n\n    Origin: Adapted from EdoardoBotta-Gaussian-Mixture-VAE (reimplemented).\n    Paper: Jang et al. "Categorical Reparameterization with Gumbel-Softmax".\n\n    Args:\n        logits: [..., K] unnormalized log-probabilities for K categories.\n        temperature: Softmax temperature.\n        device: device for sampling noise.\n\n    Returns:\n        Softmax probabilities of shape [..., K].\n    """\n    gumbel_noise = -torch.log(-torch.log(torch.randn_like(logits).to(device) + 1e-20) + 1e-20)\n    y = logits + gumbel_noise\n    return F.softmax(y / temperature, dim=-1)\n\n\ndef measure_perplexity(predicted_indices: torch.Tensor, n_embed: int) -> Tuple[torch.Tensor, torch.Tensor]:\n    """\n    Evaluate cluster perplexity and usage.\n\n    Origin: Adapted from a public implementation note by Andrej Karpathy (reimplemented).\n    Reference repo mentioned in survey notes.\n\n    Args:\n        predicted_indices: Long tensor of shape [N] containing selected code indices.\n        n_embed: Number of embeddings in the codebook.\n\n    Returns:\n        (perplexity, cluster_use):\n         - perplexity: measures how uniformly codes are used. Max equals n_embed.\n         - cluster_use: number of clusters with non-zero usage in the batch.\n    """\n    encodings = F.one_hot(predicted_indices, n_embed).float().reshape(-1, n_embed)\n    avg_probs = encodings.mean(0)\n    perplexity = (-(avg_probs * torch.log(avg_probs + 1e-10)).sum()).exp()\n    cluster_use = torch.sum(avg_probs > 0)\n    return perplexity, cluster_use\n\n\ndef householder_apply(v: torch.Tensor, w: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n    """\n    Apply the Householder transformation that maps unit vector v to unit vector w, to vector x.\n\n    We use the classic construction:\n      u = (v - w) / ||v - w||, H = I - 2 u u^T, and H v = w.\n    This avoids explicit construction of H and applies it via vector operations.\n\n    Args:\n        v: [..., D] unit vectors (encoder direction).\n        w: [..., D] unit vectors (codebook direction).\n        x: [..., D] vector to transform.\n\n    Returns:\n        H x where H maps v -> w.\n    """\n    # If v == w (or nearly), H becomes identity. We guard against numerical issues.\n    u = v - w\n    # Norm with small epsilon to avoid division by zero.\n    u_norm = torch.norm(u, dim=-1, keepdim=True).clamp_min(1e-8)\n    u = u / u_norm\n    # Householder reflection: x - 2 u (u^T x)\n    ux = torch.sum(u * x, dim=-1, keepdim=True)\n    hx = x - 2.0 * u * ux\n    return hx\n\n\nclass RRTStraightThrough(torch.autograd.Function):\n    """\n    Custom autograd to transport gradients through the quantization layer using\n    Rotation and Rescaling Transform (RRT) while keeping the forward output equal\n    to the nearest codebook vector.\n\n    Forward:\n        z_e -> e_q (nearest code vector) treated as quantized output to decoder.\n    Backward:\n        Let R be Householder reflection mapping z_dir to e_dir, and s = ||e||/||z||.\n        We transport the gradient g via s * R^T g = s * R g (R is symmetric), which\n        preserves angle with the codebook direction and provides meaningful signals\n        to the encoder.\n\n    Important: The rotation and rescaling are treated as constants w.r.t gradients,\n    i.e., we do not backpropagate into the codebook by this path (EMA handles updates).\n    """\n\n    @staticmethod\n    def forward(ctx, z_e: torch.Tensor, e_q: torch.Tensor) -> torch.Tensor:\n        # Compute per-vector unit directions and rescaling factor s.\n        # Shapes: [..., D]\n        z_norm = torch.norm(z_e, dim=-1, keepdim=True).clamp_min(1e-8)\n        e_norm = torch.norm(e_q, dim=-1, keepdim=True).clamp_min(1e-8)\n        z_dir = z_e / z_norm\n        e_dir = e_q / e_norm\n        s = (e_norm / z_norm).detach()  # stop gradient through s\n\n        # Save for backward (as detached constants)\n        ctx.save_for_backward(z_dir.detach(), e_dir.detach(), s)\n        # Forward output equals quantized vector e_q.\n        return e_q\n\n    @staticmethod\n    def backward(ctx, grad_output: torch.Tensor):\n        z_dir, e_dir, s = ctx.saved_tensors\n        # Apply Householder reflection that maps z_dir -> e_dir to grad_output.\n        # Preserve angle with codebook direction by transporting via R.\n        # R is symmetric, so R^T = R.\n        transformed = householder_apply(z_dir, e_dir, grad_output)\n        grad_z = s * transformed\n        # No gradient flows to e_q (EMA updates codebook). We return grad for z_e only.\n        return grad_z, None\n'}, {'path': 'model/vqvae.py', 'content': '"""\nVQ-VAE architecture integrating the novel Rotation-Rescaling Transform in the quantizer.\n\nComponents:\n- Encoder/Decoder CNN suitable for CIFAR-10 32x32 images\n- VectorQuantizerEMA with RRT straight-through gradients\n\nThis implementation is self-contained and does not import from external repositories.\n"""\nfrom typing import Tuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .vector_quantizer import VectorQuantizerEMA\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels: int = 3, hidden: int = 128, latent_dim: int = 64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, hidden, 4, 2, 1),  # 32->16\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden, hidden, 4, 2, 1),  # 16->8\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden, hidden, 4, 2, 1),  # 8->4\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden, latent_dim, 3, 1, 1),  # keep 4x4 latent grid\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\nclass Decoder(nn.Module):\n    def __init__(self, out_channels: int = 3, hidden: int = 128, latent_dim: int = 64):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(latent_dim, hidden, 3, 1, 1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hidden, hidden, 4, 2, 1),  # 4->8\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hidden, hidden, 4, 2, 1),  # 8->16\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hidden, out_channels, 4, 2, 1),  # 16->32\n            nn.Sigmoid(),  # normalize to [0,1]\n        )\n\n    def forward(self, z_q: torch.Tensor) -> torch.Tensor:\n        return self.net(z_q)\n\nclass VQVAE(nn.Module):\n    def __init__(self, img_channels: int = 3, codebook_size: int = 1024, beta: float = 0.25,\n                 embedding_dim: int = 64, ema_decay: float = 0.99, use_rrt: bool = True):\n        super().__init__()\n        self.encoder = Encoder(in_channels=img_channels, latent_dim=embedding_dim)\n        self.quantizer = VectorQuantizerEMA(embedding_dim=embedding_dim,\n                                            num_embeddings=codebook_size,\n                                            decay=ema_decay,\n                                            beta=beta,\n                                            use_rrt=use_rrt)\n        self.decoder = Decoder(out_channels=img_channels, latent_dim=embedding_dim)\n\n    def forward(self, x: torch.Tensor):\n        z_e = self.encoder(x)\n        z_q, indices, vq_loss, stats = self.quantizer(z_e)\n        x_rec = self.decoder(z_q)\n        return x_rec, vq_loss, stats\n'}, {'path': 'training/train.py', 'content': '"""\nTraining loop for VQ-VAE with Rotation-Rescaling Transform quantizer.\n\nImplements:\n- Reconstruction loss (MSE)\n- VQ commitment+codebook loss\n- Logging of perplexity and cluster usage\n- GPU support and OOM handling\n"""\nfrom typing import Dict\nimport torch\nimport torch.nn.functional as F\nfrom torch.cuda.amp import autocast, GradScaler\n\n\ndef train_one_epoch(model, loader, optimizer, device: torch.device) -> Dict[str, float]:\n    model.train()\n    scaler = GradScaler(enabled=torch.cuda.is_available())\n    total_rec = 0.0\n    total_vq = 0.0\n    total_loss = 0.0\n    total_ppl = 0.0\n    total_cuse = 0.0\n    n_batches = 0\n    for batch in loader:\n        x, _ = batch\n        x = x.to(device, non_blocking=True)\n        optimizer.zero_grad(set_to_none=True)\n        try:\n            with autocast(enabled=torch.cuda.is_available()):\n                x_rec, vq_loss, stats = model(x)\n                rec_loss = F.mse_loss(x_rec, x)\n                loss = rec_loss + vq_loss\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        except RuntimeError as e:\n            if \'out of memory\' in str(e).lower():\n                torch.cuda.empty_cache()\n                continue\n            else:\n                raise e\n        total_rec += rec_loss.item()\n        total_vq += vq_loss.item()\n        total_loss += loss.item()\n        total_ppl += stats[0].item()\n        total_cuse += stats[1].item()\n        n_batches += 1\n    return {\n        \'rec_loss\': total_rec / max(n_batches, 1),\n        \'vq_loss\': total_vq / max(n_batches, 1),\n        \'loss\': total_loss / max(n_batches, 1),\n        \'perplexity\': total_ppl / max(n_batches, 1),\n        \'cluster_use\': total_cuse / max(n_batches, 1),\n    }\n'}, {'path': 'testing/fid.py', 'content': '"""\nFID evaluation utilities for CIFAR-10.\n\nComputes Frechet Inception Distance between generated reconstructions and real images.\nUses torchvision\'s InceptionV3 to compute pool3 (2048-d) activations via feature extractor.\n\nThe CIFAR-10 reference statistics path is provided by the instruction, but to keep\nthis project self-contained, we compute real statistics on the test set at runtime.\n"""\nfrom typing import Tuple\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision\nimport torchvision.transforms as T\nimport numpy as np\nfrom torchvision.models.feature_extraction import create_feature_extractor\n\n\ndef get_inception_features(loader: DataLoader, device: torch.device) -> torch.Tensor:\n    """\n    Extract InceptionV3 avgpool features (pool3, 2048-d) for all images in the loader.\n    """\n    inception = torchvision.models.inception_v3(pretrained=True, transform_input=False).to(device)\n    inception.eval()\n    extractor = create_feature_extractor(inception, {"avgpool": "feat"})\n\n    normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    all_feats = []\n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(device)\n            # Inception expects 299x299\n            x = F.interpolate(x, size=(299, 299), mode="bilinear", align_corners=False)\n            x = normalize(x)\n            out = extractor(x)\n            feats = out["feat"].view(x.size(0), -1)  # [B, 2048]\n            all_feats.append(feats.detach().cpu())\n    return torch.cat(all_feats, dim=0)\n\n\ndef _sqrtm_product(sigma1: np.ndarray, sigma2: np.ndarray) -> np.ndarray:\n    """Compute matrix square root of sigma1 @ sigma2 using scipy-like fallback via eigh.\n    """\n    # Try scipy if available\n    try:\n        from scipy.linalg import sqrtm  # type: ignore\n        covmean = sqrtm(sigma1.dot(sigma2))\n        # Numeric stability: ensure real part\n        if np.iscomplexobj(covmean):\n            covmean = covmean.real\n        return covmean\n    except Exception:\n        # Fallback using eigen decomposition (approximate)\n        w1, v1 = np.linalg.eigh(sigma1)\n        w2, v2 = np.linalg.eigh(sigma2)\n        # Construct square roots of positive eigenvalues\n        s1_half = v1 @ np.diag(np.sqrt(np.clip(w1, a_min=0, a_max=None))) @ v1.T\n        s2_half = v2 @ np.diag(np.sqrt(np.clip(w2, a_min=0, a_max=None))) @ v2.T\n        return s1_half @ s2_half\n\n\ndef calculate_fid(feats1: torch.Tensor, feats2: torch.Tensor) -> float:\n    """Compute FrÃ©chet distance between two multivariate Gaussians parameterized by sample means/covariances.\n    """\n    mu1 = feats1.mean(0).numpy()\n    mu2 = feats2.mean(0).numpy()\n    sigma1 = np.cov(feats1.numpy(), rowvar=False)\n    sigma2 = np.cov(feats2.numpy(), rowvar=False)\n    diff = mu1 - mu2\n\n    covmean = _sqrtm_product(sigma1, sigma2)\n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n    return float(fid)\n'}, {'path': 'testing/test.py', 'content': '"""\nTesting procedures for VQ-VAE.\n\nComputes reconstruction metrics (MSE, PSNR, SSIM) and FID on CIFAR-10 test set.\n"""\nfrom typing import Dict\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom .fid import get_inception_features, calculate_fid\n\ndef psnr_from_mse(mse: torch.Tensor, max_val: float = 1.0) -> float:\n    # PSNR in dB\n    import math\n    if mse <= 0:\n        return float(\'inf\')\n    return 10.0 * math.log10((max_val ** 2) / float(mse))\n\ndef ssim_simple(x: torch.Tensor, y: torch.Tensor, C1: float = 0.01**2, C2: float = 0.03**2) -> float:\n    """\n    A simple SSIM approximation computed per-image over the whole frame (no sliding window).\n    Inputs expected in [0,1]. Returns average SSIM over batch.\n    """\n    # Flatten spatial dims\n    B = x.size(0)\n    x_flat = x.view(B, x.size(1), -1)\n    y_flat = y.view(B, y.size(1), -1)\n    mu_x = x_flat.mean(-1)\n    mu_y = y_flat.mean(-1)\n    var_x = x_flat.var(-1, unbiased=False)\n    var_y = y_flat.var(-1, unbiased=False)\n    cov_xy = ((x_flat - mu_x.unsqueeze(-1)) * (y_flat - mu_y.unsqueeze(-1))).mean(-1)\n    ssim_c = ((2 * mu_x * mu_y + C1) * (2 * cov_xy + C2)) / ((mu_x ** 2 + mu_y ** 2 + C1) * (var_x + var_y + C2))\n    # Average over channels then batch\n    return float(ssim_c.mean().item())\n\ndef evaluate(model, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n    model.eval()\n    total_rec = 0.0\n    n_batches = 0\n    orig_images = []\n    rec_images = []\n    with torch.no_grad():\n        for x, _ in loader:\n            x = x.to(device)\n            x_rec, vq_loss, stats = model(x)\n            rec_loss = F.mse_loss(x_rec, x)\n            total_rec += rec_loss.item()\n            n_batches += 1\n            orig_images.append(x.detach().cpu())\n            rec_images.append(x_rec.detach().cpu())\n    # Stack and compute FID\n    orig = torch.cat(orig_images, dim=0)\n    rec = torch.cat(rec_images, dim=0)\n    # Build loaders for feature extraction\n    from torch.utils.data import TensorDataset\n    orig_loader = torch.utils.data.DataLoader(TensorDataset(orig, torch.zeros(orig.size(0))), batch_size=64)\n    rec_loader = torch.utils.data.DataLoader(TensorDataset(rec, torch.zeros(rec.size(0))), batch_size=64)\n    feats_orig = get_inception_features(orig_loader, device)\n    feats_rec = get_inception_features(rec_loader, device)\n    fid = calculate_fid(feats_orig, feats_rec)\n    # PSNR/SSIM\n    avg_mse = total_rec / max(n_batches, 1)\n    psnr = psnr_from_mse(avg_mse)\n    ssim = ssim_simple(orig, rec)\n    return {\n        \'rec_loss\': avg_mse,\n        \'psnr\': psnr,\n        \'ssim\': ssim,\n        \'fid\': fid,\n    }\n'}]